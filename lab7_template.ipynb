{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhouziyuanjudy/PHYS417SP25/blob/main/lab7_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "581f5f99",
      "metadata": {
        "id": "581f5f99"
      },
      "source": [
        "# Lab 7 Template"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d682e0d0",
      "metadata": {
        "id": "d682e0d0"
      },
      "source": [
        "## Preliminary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a4a3fe1",
      "metadata": {
        "id": "8a4a3fe1"
      },
      "source": [
        "### Packages to install\n",
        "\n",
        "- ucimlrepo (for grabbing datasets)\n",
        "- transformers\n",
        "- autogen\n",
        "- jupyter (if you aren't on the latest version, there's a dependency in tqdm that complains)\n",
        "\n",
        "#### Install with pip\n",
        "\n",
        "- pip install ucimlrepo transformers autogen jupyter\n",
        "\n",
        "### From lab 3\n",
        "\n",
        "- pandas, numpy, matplotlib.pyplot, seaborn, tqdm, torch, sklearn\n",
        "\n",
        "#### Install everything with pip\n",
        "\n",
        "- pip install ucimlrepo transformers autogen pandas numpy matplotlib seaborn tqdm torch scikit-learn jupyter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "eac90980",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "eac90980",
        "outputId": "b57e2042-dcad-4d86-aa22-26b777366356"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ucimlrepo\n",
            "  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting autogen\n",
            "  Downloading autogen-0.9.1.post0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting jupyter\n",
            "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo) (2025.4.26)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Collecting ag2==0.9.1post0 (from autogen)\n",
            "  Downloading ag2-0.9.1.post0-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: anyio<5.0.0,>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from ag2==0.9.1post0->autogen) (4.9.0)\n",
            "Collecting asyncer==0.0.8 (from ag2==0.9.1post0->autogen)\n",
            "  Downloading asyncer-0.0.8-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting diskcache (from ag2==0.9.1post0->autogen)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting docker (from ag2==0.9.1post0->autogen)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from ag2==0.9.1post0->autogen) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from ag2==0.9.1post0->autogen) (2.11.4)\n",
            "Collecting python-dotenv (from ag2==0.9.1post0->autogen)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from ag2==0.9.1post0->autogen) (3.1.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from ag2==0.9.1post0->autogen) (0.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.11/dist-packages (from jupyter) (6.5.7)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.11/dist-packages (from jupyter) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.11/dist-packages (from jupyter) (7.16.6)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from jupyter) (6.17.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (from jupyter) (7.7.1)\n",
            "Collecting jupyterlab (from jupyter)\n",
            "  Downloading jupyterlab-4.4.2-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (1.8.0)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (6.4.2)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (5.7.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter) (3.0.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-console->jupyter) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from jupyter-console->jupyter) (2.19.1)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter)\n",
            "  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter) (5.7.2)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter)\n",
            "  Downloading jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->jupyter)\n",
            "  Downloading jupyter_server-2.16.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->jupyter)\n",
            "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter) (0.2.4)\n",
            "Requirement already satisfied: setuptools>=41.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter) (75.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (4.13.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (0.10.2)\n",
            "Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (5.10.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (1.5.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter) (23.1.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=3.0.0->ag2==0.9.1post0->autogen) (1.3.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (1.4.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.28.1->ag2==0.9.1post0->autogen) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.28.1->ag2==0.9.1post0->autogen) (0.16.0)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel->jupyter)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.9.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core->jupyterlab->jupyter) (4.3.8)\n",
            "Collecting jupyter-client>=6.1.12 (from ipykernel->jupyter)\n",
            "  Downloading jupyter_client-7.4.9-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
            "  Downloading jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
            "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook->jupyter) (21.2.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (0.4)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.17.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter)\n",
            "  Downloading json5-0.12.0-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (4.23.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7->nbconvert->jupyter) (2.21.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter) (0.2.13)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6.1->ag2==0.9.1post0->autogen) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6.1->ag2==0.9.1post0->autogen) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6.1->ag2==0.9.1post0->autogen) (0.4.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.11/dist-packages (from terminado>=0.8.3->notebook->jupyter) (0.7.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert->jupyter) (2.7)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter) (0.8.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.24.0)\n",
            "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
            "  Downloading python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (2.22)\n",
            "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (3.0.0)\n",
            "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (24.11.1)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
            "  Downloading types_python_dateutil-2.9.0.20250516-py3-none-any.whl.metadata (2.1 kB)\n",
            "Downloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\n",
            "Downloading autogen-0.9.1.post0-py3-none-any.whl (13 kB)\n",
            "Downloading ag2-0.9.1.post0-py3-none-any.whl (787 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m787.5/787.5 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asyncer-0.0.8-py3-none-any.whl (9.2 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m841.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
            "Downloading jupyterlab-4.4.2-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_server-2.16.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.9/386.9 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_client-7.4.9-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.5/133.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json5-0.12.0-py3-none-any.whl (36 kB)\n",
            "Downloading jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
            "Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
            "Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20250516-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: uri-template, types-python-dateutil, rfc3986-validator, rfc3339-validator, python-json-logger, python-dotenv, overrides, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, json5, jedi, fqdn, diskcache, async-lru, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jupyter-server-terminals, jupyter-client, docker, asyncer, arrow, ucimlrepo, nvidia-cusolver-cu12, isoduration, ag2, autogen, jupyter-events, jupyter-server, jupyterlab-server, jupyter-lsp, jupyterlab, jupyter\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 6.1.12\n",
            "    Uninstalling jupyter-client-6.1.12:\n",
            "      Successfully uninstalled jupyter-client-6.1.12\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: jupyter-server\n",
            "    Found existing installation: jupyter-server 1.16.0\n",
            "    Uninstalling jupyter-server-1.16.0:\n",
            "      Successfully uninstalled jupyter-server-1.16.0\n",
            "Successfully installed ag2-0.9.1.post0 arrow-1.3.0 async-lru-2.0.5 asyncer-0.0.8 autogen-0.9.1.post0 diskcache-5.6.3 docker-7.1.0 fqdn-1.5.1 isoduration-20.11.0 jedi-0.19.2 json5-0.12.0 jupyter-1.1.1 jupyter-client-7.4.9 jupyter-events-0.12.0 jupyter-lsp-2.2.5 jupyter-server-2.16.0 jupyter-server-terminals-0.5.3 jupyterlab-4.4.2 jupyterlab-server-2.27.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 overrides-7.7.0 python-dotenv-1.1.0 python-json-logger-3.3.0 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 types-python-dateutil-2.9.0.20250516 ucimlrepo-0.0.7 uri-template-1.3.0\n"
          ]
        }
      ],
      "source": [
        "# If not already done in Colab, install dependencies\n",
        "# !pip install ucimlrepo transformers autogen pandas numpy matplotlib seaborn tqdm torch scikit-learn jupyter\n",
        "!pip install ucimlrepo transformers autogen pandas numpy matplotlib seaborn tqdm torch scikit-learn jupyter\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Core imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tqdm\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# LLM and AutoGen\n",
        "from autogen import ConversableAgent, AssistantAgent\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b54903e1",
      "metadata": {
        "id": "b54903e1"
      },
      "source": [
        "### Intro\n",
        "\n",
        "The goal of this lab is to give you an idea of how you could use agents to help with physics tasks. It will also introduce you to AutoGen, one of the more popular frameworks at the moment for designing custom agentic workflows. We won't make use of all the tools it provides, just the very basics. Note also that many of the most interesting things one can do with AI-powered agents (see topics like retreival augmented generation (RAG)) require very large models to be performant, and many techniques require continuous/repeated training. This means large resource requirements, so for this lab we will just be using a very small LLM (Llama: TinyLlama-1.1B-Chat-v1.0). The results from this are nowhere near as good as something like chatGPT, but it should give you an idea of how a more advanced model (or models) might be able to do something really helpful/cool. This is an area of current research, so it will be interesting to see what they can do!\n",
        "\n",
        "Also because of the limited size of the model, the text parsing needs to be very mechanical, and in some places a bit obtuse. The better (and ideally specially trained for an agentic workflow, see RAG) your model is, the more this can be relaxed. If you're interested in working with agents in a more user-friendly way (hiding a lot of the mechanics that are on display here), check out sites like n8n or Google Gemini (be aware that these can require you to provide a lot of permissions). You can also feel free to replace TinyLlama with a call to a larger model using API keys if you have some.\n",
        "\n",
        "Finally, we would like to emphasize the use of copilot/similar tools for this lab in particular. These are agents too! And they are definitely the most performant agents you have easy access to.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d3f282e",
      "metadata": {
        "id": "1d3f282e"
      },
      "source": [
        "## Lab"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68bad3ed",
      "metadata": {
        "id": "68bad3ed"
      },
      "source": [
        "- There are two datasets, \"mnist\" (from Lab 3) and \"solar_flare\"\n",
        "- Try to get everything working with mnist, then try adding solar_flare\n",
        "    - This is one way agents can be helpful, since they can analyze a dataset you've never seen before and take a first crack at it much faster than you can"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d391c968",
      "metadata": {
        "id": "d391c968"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "835b6358",
      "metadata": {
        "id": "835b6358",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "69048595-2cb4-4f6e-f609-86de0223029c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Import necessary packages\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Agentic AI framework and LLM tools\n",
        "from autogen import ConversableAgent, AssistantAgent\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tqdm\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Dataset repository\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "# Check GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e85da87",
      "metadata": {
        "id": "5e85da87"
      },
      "source": [
        "### Lab 3 code (run and then minimize this)\n",
        "\n",
        "- Mostly just lab 3 code packaged into functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "1f462f1c",
      "metadata": {
        "id": "1f462f1c"
      },
      "outputs": [],
      "source": [
        "# Load MNIST Dataset in Numpy\n",
        "def load_mnist_data():\n",
        "    # 1000 training samples where each sample feature is a greyscale image with shape (28, 28)\n",
        "    # 1000 training targets where each target is an integer indicating the true digit\n",
        "    mnist_train_features = np.load('mnist_train_features.npy')\n",
        "    mnist_train_targets = np.load('mnist_train_targets.npy')\n",
        "\n",
        "    # 100 testing samples + targets\n",
        "    mnist_test_features = np.load('mnist_test_features.npy')\n",
        "    mnist_test_targets = np.load('mnist_test_targets.npy')\n",
        "\n",
        "    # Print the dimensions of training sample features/targets\n",
        "    #print(mnist_train_features.shape, mnist_train_targets.shape)\n",
        "    # Print the dimensions of testing sample features/targets\n",
        "    #print(mnist_test_features.shape, mnist_test_targets.shape)\n",
        "\n",
        "    return mnist_train_features, mnist_train_targets, mnist_test_features, mnist_test_targets\n",
        "\n",
        "\n",
        "def flatten_features(features):\n",
        "    # Flatten the features from (28, 28) to (784,)\n",
        "    return features.reshape(features.shape[0], -1)\n",
        "\n",
        "def scale_features(features):\n",
        "    scaler = StandardScaler()\n",
        "    return scaler.fit_transform(features)\n",
        "\n",
        "# More general function to load datasets, including solar_flare\n",
        "def load_dataset(dataset_name: str = \"\"):\n",
        "    if dataset_name == \"mnist\":\n",
        "        train_features, train_targets, test_features, test_targets = load_mnist_data()\n",
        "        train_features = flatten_features(train_features)\n",
        "        test_features = flatten_features(test_features)\n",
        "        train_features = scale_features(train_features)\n",
        "        test_features = scale_features(test_features)\n",
        "\n",
        "    elif dataset_name == \"solar_flare\":\n",
        "        # Load the solar flare dataset\n",
        "        solar_flare = fetch_ucirepo(id=89)\n",
        "\n",
        "        # Simplifying slightly for the sake of this example\n",
        "        solar_flare.data.targets = solar_flare.data.targets['severe flares']\n",
        "\n",
        "        # Split the solar flare dataset into train and test sets (90:10 split)\n",
        "        train_features, test_features, train_targets, test_targets = train_test_split(\n",
        "            solar_flare.data.features, solar_flare.data.targets, test_size=0.1, random_state=42\n",
        "        )\n",
        "\n",
        "        # Onehot encode modified Zurich class, largest spot size, spot distribution\n",
        "        onehot_columns = [\"modified Zurich class\", \"largest spot size\", \"spot distribution\"]\n",
        "        for col in onehot_columns:\n",
        "            onehot = pd.get_dummies(train_features[col], prefix=col)\n",
        "            train_features = pd.concat([train_features, onehot], axis=1)\n",
        "            train_features.drop(col, axis=1, inplace=True)\n",
        "\n",
        "            onehot = pd.get_dummies(test_features[col], prefix=col)\n",
        "            test_features = pd.concat([test_features, onehot], axis=1)\n",
        "            test_features.drop(col, axis=1, inplace=True)\n",
        "\n",
        "        # Scale the features\n",
        "        train_features = scale_features(train_features)\n",
        "        test_features = scale_features(test_features)\n",
        "\n",
        "        # Convert targets to numpy arrays\n",
        "        train_targets = train_targets.to_numpy()\n",
        "        test_targets = test_targets.to_numpy()\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
        "\n",
        "    # train-test split\n",
        "    train_features, val_features, train_targets, val_targets = train_test_split(train_features, train_targets, test_size=0.2)\n",
        "\n",
        "    return train_features, train_targets, val_features, val_targets, test_features, test_targets\n",
        "\n",
        "\n",
        "# Train\n",
        "def train_model(model, train_features, train_targets, validation_features, validation_targets,\n",
        "                test_features=None, test_targets=None, learning_rate=0.0015, epochs=80, batch_size=64):\n",
        "    \"\"\"\n",
        "    Train a neural network model on the provided data.\n",
        "\n",
        "    Parameters:\n",
        "        model: PyTorch model to train\n",
        "        train_features: Training features as numpy array\n",
        "        train_targets: Training targets as numpy array\n",
        "        validation_features: Validation features as numpy array\n",
        "        validation_targets: Validation targets as numpy array\n",
        "        test_features: Test features as numpy array (optional)\n",
        "        test_targets: Test targets as numpy array (optional)\n",
        "        learning_rate: Learning rate for optimizer\n",
        "        epochs: Number of training epochs\n",
        "        batch_size: Batch size for training\n",
        "\n",
        "    Returns:\n",
        "        tuple: (trained model, training loss list, validation accuracy list)\n",
        "    \"\"\"\n",
        "    # Initialize tracking lists\n",
        "    train_loss_list = np.zeros(epochs)\n",
        "    validation_accuracy_list = np.zeros(epochs)\n",
        "\n",
        "    # Convert numpy arrays to PyTorch tensors\n",
        "    train_inputs = torch.from_numpy(train_features).float()\n",
        "    train_targets = torch.from_numpy(train_targets).long()\n",
        "\n",
        "    validation_inputs = torch.from_numpy(validation_features).float()\n",
        "    validation_targets = torch.from_numpy(validation_targets).long()\n",
        "\n",
        "    if test_features is not None and test_targets is not None:\n",
        "        test_inputs = torch.from_numpy(test_features).float()\n",
        "        test_targets = torch.from_numpy(test_targets).long()\n",
        "        test_dataset = TensorDataset(test_inputs, test_targets)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_dataset = TensorDataset(train_inputs, train_targets)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    validation_dataset = TensorDataset(validation_inputs, validation_targets)\n",
        "    validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Setup optimizer and scheduler\n",
        "    loss_func = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "    # Move model to GPU if available\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "        train_inputs = train_inputs.cuda()\n",
        "        validation_inputs = validation_inputs.cuda()\n",
        "        validation_targets = validation_targets.cuda()\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in tqdm.trange(epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for batch_inputs, batch_targets in train_loader:\n",
        "            if torch.cuda.is_available():\n",
        "                batch_inputs, batch_targets = batch_inputs.cuda(), batch_targets.cuda()\n",
        "\n",
        "            optimizer.zero_grad()  # Reset gradients to zero\n",
        "            outputs = model(batch_inputs)  # Forward pass with current batch\n",
        "            loss = loss_func(outputs, batch_targets)  # Compute loss\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Update weights\n",
        "\n",
        "            running_loss += loss.item() * batch_inputs.size(0)\n",
        "\n",
        "        # Store average epoch loss\n",
        "        train_loss_list[epoch] = running_loss / len(train_dataset)\n",
        "        scheduler.step()  # Update learning rate with cosine annealing\n",
        "\n",
        "        # Compute Validation Accuracy\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for val_inputs, val_targets in validation_loader:\n",
        "                if torch.cuda.is_available():\n",
        "                    val_inputs, val_targets = val_inputs.cuda(), val_targets.cuda()\n",
        "                outputs = model(val_inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += val_targets.size(0)\n",
        "                correct += (predicted == val_targets).sum().item()\n",
        "\n",
        "            validation_accuracy_list[epoch] = correct / total\n",
        "\n",
        "    # Compute test accuracy if test data is provided\n",
        "    test_accuracy = None\n",
        "    if test_features is not None and test_targets is not None:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for test_inputs, test_targets in test_loader:\n",
        "                if torch.cuda.is_available():\n",
        "                    test_inputs, test_targets = test_inputs.cuda(), test_targets.cuda()\n",
        "                outputs = model(test_inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += test_targets.size(0)\n",
        "                correct += (predicted == test_targets).sum().item()\n",
        "\n",
        "            test_accuracy = correct / total\n",
        "\n",
        "    return model, train_loss_list, validation_accuracy_list, test_accuracy\n",
        "\n",
        "\n",
        "# Visualize and evaluate\n",
        "def visualize_training(train_loss_list, validation_accuracy_list):\n",
        "    \"\"\"\n",
        "    Visualize training loss and validation accuracy.\n",
        "\n",
        "    Parameters:\n",
        "        train_loss_list: List of training losses\n",
        "        validation_accuracy_list: List of validation accuracies\n",
        "    \"\"\"\n",
        "    plt.figure(figsize = (12, 6))\n",
        "\n",
        "    # Visualize training loss with respect to iterations (1 iteration -> single batch)\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(train_loss_list, linewidth = 3)\n",
        "    plt.ylabel(\"training loss\")\n",
        "    plt.xlabel(\"epochs\")\n",
        "    sns.despine()\n",
        "\n",
        "    # Visualize validation accuracy with respect to epochs\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(validation_accuracy_list, linewidth = 3, color = 'gold')\n",
        "    plt.ylabel(\"validation accuracy\")\n",
        "    sns.despine()\n",
        "\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86c39b73",
      "metadata": {
        "id": "86c39b73"
      },
      "source": [
        "## Define Model(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc0e4975",
      "metadata": {
        "id": "bc0e4975"
      },
      "source": [
        "- Here you should make two models, one linear and one convolutional. The agents will use these later.\n",
        "- Copilot/similar tools are encouraged for this; remember that they are agents too!\n",
        "- Also you can add more if you want\n",
        "- Also make sure to reshape the input data to the correct shape. Each model might be given mnist data or later solar_flare data. But these have different shapes! Mnist is AxBx1, but solar_flare is just Ax1\n",
        "    - You don't need to perfect this on your first pass through, just make sure both with mnist and you can revisit later"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LinearClassification (for flattened MNIST)\n",
        "class LinearClassification(torch.nn.Module):\n",
        "    def __init__(self, input_dim=784, output_dim=10):\n",
        "        super(LinearClassification, self).__init__()\n",
        "        self.description = \"Simple linear model for flattened input\"\n",
        "        self.fc1 = torch.nn.Linear(input_dim, 200)\n",
        "        self.dropout = torch.nn.Dropout(0.2)\n",
        "        self.fc2 = torch.nn.Linear(200, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Input x should be a 2D tensor with shape (number of samples, number of features) (N, D)\n",
        "        MNIST: (N, 784)\n",
        "        swap out mnist for solar_flare to get the correct shape\n",
        "        Solar Flare: (N, #features)\n",
        "        \"\"\"\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Convolutional model for image-shaped MNIST (28x28 grayscale)\n",
        "class ConvClassification(torch.nn.Module):\n",
        "    def __init__(self, output_dim=10):\n",
        "        super(ConvClassification, self).__init__()\n",
        "        self.description = \"Simple CNN for MNIST\"\n",
        "        self.conv1 = torch.nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = torch.nn.MaxPool2d(2, 2)\n",
        "        self.dropout = torch.nn.Dropout(0.25)\n",
        "        self.fc1 = torch.nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = torch.nn.Linear(128, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Expects x with shape (N, 1, 28, 28)\n",
        "        \"\"\"\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "9fsSkQJXQObe"
      },
      "id": "9fsSkQJXQObe",
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "aa1b73ea",
      "metadata": {
        "id": "aa1b73ea"
      },
      "source": [
        "# Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "696d2898",
      "metadata": {
        "id": "696d2898"
      },
      "source": [
        "### DatasetLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d9f9976",
      "metadata": {
        "id": "2d9f9976"
      },
      "source": [
        "Here is the definition of the agent that handles dataset loading. The logic is pretty simple, so nothing for you to code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "f0ce5141",
      "metadata": {
        "id": "f0ce5141"
      },
      "outputs": [],
      "source": [
        "class DatasetLoaderAgent(AssistantAgent):\n",
        "    \"\"\"\n",
        "    An agent that specializes in loading and preprocessing datasets.\n",
        "    \"\"\"\n",
        "    def __init__(self, name=\"DatasetLoader\", **kwargs):\n",
        "        self.available_datasets = [\"mnist\", \"solar_flare\"]\n",
        "        system_message = (\n",
        "            \"I am a dataset loading assistant. I can load and preprocess various datasets for machine learning tasks. \"\n",
        "            f\"Currently I support: {', '.join(self.available_datasets)}. \"\n",
        "        )\n",
        "        super().__init__(name=name, system_message=system_message, **kwargs)\n",
        "\n",
        "        # Store loaded datasets\n",
        "        self._loaded_datasets = {}\n",
        "\n",
        "    def get_available_datasets(self):\n",
        "        \"\"\"Return a list of available datasets.\"\"\"\n",
        "        return self.available_datasets\n",
        "\n",
        "    def load_dataset(self, dataset_name):\n",
        "        \"\"\"\n",
        "        Load and preprocess a dataset.\n",
        "\n",
        "        Args:\n",
        "            dataset_name (str): Name of the dataset to load\n",
        "\n",
        "        Returns:\n",
        "            dict: Information about the loaded dataset and the data itself\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Load the dataset using the existing function\n",
        "            train_features, train_targets, val_features, val_targets, test_features, test_targets = load_dataset(dataset_name)\n",
        "\n",
        "            # Store the dataset\n",
        "            self._loaded_datasets[dataset_name] = {\n",
        "                \"train_features\": train_features,\n",
        "                \"train_targets\": train_targets,\n",
        "                \"validation_features\": val_features,\n",
        "                \"validation_targets\": val_targets,\n",
        "                \"test_features\": test_features,\n",
        "                \"test_targets\": test_targets,\n",
        "            }\n",
        "            # print out result for testing\n",
        "            print(f\"Loaded dataset: {dataset_name} with {train_features.shape[0]} training samples\")\n",
        "\n",
        "            # Return information about the loaded dataset\n",
        "            return {\n",
        "                \"status\": \"success\",\n",
        "                \"dataset_name\": dataset_name,\n",
        "                \"train_samples\": train_features.shape[0],\n",
        "                \"validation_samples\": val_features.shape[0],\n",
        "                \"test_samples\": test_features.shape[0],\n",
        "                \"feature_dim\": train_features.shape[1]\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"status\": \"error\",\n",
        "                \"message\": f\"Failed to load dataset '{dataset_name}': {str(e)}\"\n",
        "            }\n",
        "\n",
        "    def get_dataset(self, dataset_name):\n",
        "        \"\"\"\n",
        "        Retrieve a previously loaded dataset.\n",
        "\n",
        "        Args:\n",
        "            dataset_name (str): Name of the dataset to retrieve\n",
        "\n",
        "        Returns:\n",
        "            dict: The dataset components or None if not found\n",
        "        \"\"\"\n",
        "        return self._loaded_datasets.get(dataset_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelTrainerAgent(ConversableAgent):\n",
        "    def __init__(self, name=\"ModelTrainer\", dataset_loader_agent=None, **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.dataset_loader_agent = dataset_loader_agent\n",
        "        self.model = None\n",
        "        self.train_loss_list = None\n",
        "        self.val_accuracy_list = None\n",
        "        self.test_accuracy = None\n",
        "        self.available_models = [\"linear\", \"conv\"]\n",
        "\n",
        "    def train_model_on_query(self, dataset_name, query, override=None):\n",
        "        print(\"train_model_on_query called\")\n",
        "        print(f\"Received dataset: {dataset_name}, query: {query}, override: {override}\")\n",
        "\n",
        "        # Mock result for now\n",
        "        model_type = override or \"linear\"\n",
        "        self.model = f\"Fake {model_type} model for {dataset_name}\"\n",
        "\n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"model\": self.model,\n",
        "            \"response\": f\"Trained a {model_type} model on {dataset_name} (mock output)\"\n",
        "        }\n"
      ],
      "metadata": {
        "id": "nFJjhiuT1yt2"
      },
      "id": "nFJjhiuT1yt2",
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3ea863d2",
      "metadata": {
        "id": "3ea863d2"
      },
      "source": [
        "### InterfaceAgent"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ba0d527",
      "metadata": {
        "id": "4ba0d527"
      },
      "source": [
        "- Setting up the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "id": "2c18ca87",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2c18ca87",
        "outputId": "303c169e-b18f-4ea8-fca8-cd8b9f942242"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Set up the LLM\n",
        "\n",
        "# Load the model\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
        "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create a simple text-generation pipeline\n",
        "llm_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=200,\n",
        "    do_sample=True,\n",
        "    temperature=0.4,\n",
        ")\n",
        "\n",
        "# Wrapper function for the local model pipeline\n",
        "def local_model_generate(prompt):\n",
        "    output = llm_pipeline(prompt)[0][\"generated_text\"]\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe3ac639",
      "metadata": {
        "id": "fe3ac639"
      },
      "source": [
        "#### Agent definition\n",
        "\n",
        "- This is the definition of the actual agent you will converse with\n",
        "- Implement the query and get dataset commands\n",
        "- You can come back to the train command after you get to the model trainer agent\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Agent definition\n",
        "class InterfaceAgent(ConversableAgent):\n",
        "    def __init__(self, name, **kwargs):\n",
        "        super().__init__(name, **kwargs)\n",
        "\n",
        "        self.dataset_loader_agent = None\n",
        "        self.model_trainer_agent = None\n",
        "\n",
        "    def set_dataset_loader_agent(self, agent):\n",
        "        self.dataset_loader_agent = agent\n",
        "\n",
        "    def set_model_trainer_agent(self, agent):\n",
        "        self.model_trainer_agent = agent\n",
        "\n",
        "    def generate_reply(self, messages):\n",
        "        \"\"\"\n",
        "        There is a lot of obtuse text parsing and such here. I would describe the code as \"technically functional\".\n",
        "        This is because the LLM is so limited. With a better LLM, and/or ideally one trained explicitly for\n",
        "        agentic implementation, this would be much cleaner and more flexible.\n",
        "        C.f. retrieval augmented generation, etc.\n",
        "        \"\"\"\n",
        "        # Extract the latest user message\n",
        "        user_message = messages[-1][\"content\"]\n",
        "\n",
        "        # Check if the message is a \"get dataset\" command\n",
        "        # get dataset <dataset_name>\n",
        "        if \"get dataset\" in user_message:\n",
        "            dataset_name = user_message.replace(\"get dataset\", \"\").strip()\n",
        "\n",
        "            # Use the DatasetLoaderAgent to load the dataset\n",
        "            result = self.dataset_loader_agent.load_dataset(dataset_name)\n",
        "\n",
        "            # Check if the dataset was successfully loaded\n",
        "            if result[\"status\"] == \"success\":\n",
        "                response_text = (\n",
        "                    f\"Successfully loaded dataset '{result['dataset_name']}'.\\n\"\n",
        "                    f\"Training samples: {result['train_samples']}, \"\n",
        "                    f\"Validation samples: {result['validation_samples']}, \"\n",
        "                    f\"Test samples: {result['test_samples']}, \"\n",
        "                    f\"Feature dimension: {result['feature_dim']}.\"\n",
        "                )\n",
        "            else:\n",
        "                response_text = f\"Failed to load dataset '{dataset_name}': {result['message']}\"\n",
        "\n",
        "        # Check if the message is a \"query\" command\n",
        "        # query <dataset_name>\n",
        "        elif \"query\" in user_message:\n",
        "            query = user_message.replace(\"query\", \"\").strip()\n",
        "\n",
        "            # Use the DatasetLoaderAgent to retrieve the dataset\n",
        "            # Assuming the query is in the format \"query <dataset_name>\"\n",
        "            # In principle, this is where you could have an LLM parse the command\n",
        "            dataset_name = query.split()[0]\n",
        "            dataset = self.dataset_loader_agent.get_dataset(dataset_name)\n",
        "\n",
        "            if dataset:\n",
        "                # Prepare a prompt with dataset information\n",
        "                prompt = (\n",
        "                    f\"The dataset '{dataset_name}' has been loaded. \"\n",
        "                    f\"The first two rows of the training features are:\\n\"\n",
        "                    f\"{dataset['train_features'][:2]}.\\n\"\n",
        "                    \"Short, concise description of the dataset:\\n\"\n",
        "                )\n",
        "                # Generate a description using the local model\n",
        "                response_text = local_model_generate(prompt)\n",
        "            else:\n",
        "                response_text = f\"Dataset '{dataset_name}' is not loaded or does not exist.\"\n",
        "\n",
        "        elif \"train\" in user_message:\n",
        "            # Should be of the form \"train <dataset_name> <model_type>\"\n",
        "            # Model type\n",
        "            parts = user_message.strip().split()\n",
        "\n",
        "            if len(parts) == 3: #three word(train mnist linear)\n",
        "                dataset_name = parts[1] # parts[1] is \"mnist\" → this is the dataset name\n",
        "                modeltype = parts[2] #parts[2] is \"linear\" → this is the model type\n",
        "            elif len(parts) == 2:  #two word: train mnist\n",
        "                dataset_name = parts[1]\n",
        "                modeltype = \"random\"\n",
        "            else:\n",
        "                return {\"role\": \"assistant\", \"content\": \"Please use 'train <dataset> <modeltype>'\"}\n",
        "\n",
        "            # Use the ModelTrainerAgent to train the model\n",
        "            if self.model_trainer_agent is not None:\n",
        "                # If the user specified an available model type, use that\n",
        "                if modeltype in self.model_trainer_agent.available_models:\n",
        "                    result = self.model_trainer_agent.train_model_on_query(dataset_name, \"\", override=modeltype)\n",
        "                else:\n",
        "                    # Let the LLM describe the dataset and pick the model\n",
        "                    prompt = f\"Describe the dataset '{dataset_name}'.\"\n",
        "                    description = local_model_generate(prompt)\n",
        "                    result = self.model_trainer_agent.train_model_on_query(dataset_name, description)\n",
        "\n",
        "                if result[\"status\"] == \"success\":\n",
        "                    response_text = f\"Successfully trained the model.\"\n",
        "                else:\n",
        "                    response_text = f\"Failed to train the model: {result['response']}\"\n",
        "            else:\n",
        "                response_text = \"Model trainer agent is not set.\"\n",
        "\n",
        "        # No special command, just a normal message. Can just use the model as a chatbot\n",
        "        else:\n",
        "            response_text = local_model_generate(user_message)\n",
        "\n",
        "        # Return the response in the Autogen format\n",
        "        return {\"role\": \"assistant\", \"content\": response_text}\n"
      ],
      "metadata": {
        "id": "mBtfkeE90sYL"
      },
      "id": "mBtfkeE90sYL",
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create instances of the agents\n",
        "dataset_loader_agent = DatasetLoaderAgent(name=\"DatasetLoader\")\n",
        "model_trainer_agent = ModelTrainerAgent(name=\"ModelTrainer\", dataset_loader_agent=dataset_loader_agent)\n",
        "\n",
        "local_agent = InterfaceAgent(name=\"LocalAgent\")\n",
        "local_agent.set_model_trainer_agent(model_trainer_agent)\n",
        "local_agent.set_dataset_loader_agent(dataset_loader_agent)\n",
        "\n",
        "# Simulate a conversation\n",
        "messages = [{\"role\": \"user\", \"content\": \"get dataset mnist\"}]\n",
        "\n",
        "# InterfaceAgent processes the first message\n",
        "response = local_agent.generate_reply(messages)\n",
        "print(response[\"content\"])\n",
        "print()\n",
        "\n",
        "# Add a second message to the conversation\n",
        "messages.append({\"role\": \"user\", \"content\": \"query mnist\"})\n",
        "\n",
        "# InterfaceAgent processes the second message\n",
        "response = local_agent.generate_reply(messages)\n",
        "print(response[\"content\"])\n",
        "print()\n",
        "\n",
        "# You can also use the model as a chatbot\n",
        "messages.append({\"role\": \"user\", \"content\": \"Why don't whales have feet?\"})\n",
        "response = local_agent.generate_reply(messages)\n",
        "print(response[\"content\"])\n",
        "print()\n",
        "\n",
        "# Note that the output is not reliable at all since the model is tiny. Sometimes it's surprisingly good though\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9xm63SLx04W8",
        "outputId": "7f2539bb-9e9b-47a4-f09c-e6cde6fbd133"
      },
      "id": "9xm63SLx04W8",
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset: mnist with 800 training samples\n",
            "Successfully loaded dataset 'mnist'.\n",
            "Training samples: 800, Validation samples: 200, Test samples: 100, Feature dimension: 784.\n",
            "\n",
            "The dataset 'mnist' has been loaded. The first two rows of the training features are:\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]].\n",
            "Short, concise description of the dataset:\n",
            "The dataset'mnist' contains 60,000 28x28 grayscale images of handwritten digits, where each image is 784 pixels wide. The dataset is used for training and testing purposes. The target variable is the digit label, with 1 for a 0, 2 for a 1, and 3 for a 2. The dataset is divided into training (70,000 images), validation (10,000 images), and test (20,000 images). The dataset is used for binary classification problems.\n",
            "\n",
            "Why don't whales have feet?\n",
            "\n",
            "Narrator: Whales are the largest animals on Earth, but they have feet. In fact, they have four toes on each foot.\n",
            "\n",
            "Narrator: The first two toes are larger than the others, and they are called paddles. The third and fourth toes are smaller than the others and are used for balance.\n",
            "\n",
            "Narrator: Whales have a unique way of swimming that allows them to move quickly through the water. They use their paddles to push themselves forward, and their tail flippers to steer and control their direction.\n",
            "\n",
            "Narrator: Whales are also incredibly intelligent creatures, and they have been observed communicating with each other through vocalizations.\n",
            "\n",
            "Narrator: Whales are also important to the ecosystem. They are a crucial part of the food chain, and they help to maintain the balance of the ocean.\n",
            "\n",
            "Narrator:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c55cf5e8",
      "metadata": {
        "id": "c55cf5e8"
      },
      "source": [
        "## Model Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc78b0a6",
      "metadata": {
        "id": "bc78b0a6"
      },
      "source": [
        "This is the agent that actually trains a model for analyzing the dataset. In InterfaceAgent, the train command should generate a description of the dataset and feed it to the train_model_on_query method below (if the user didn't explicitly specify a model type). So use the LLM to generate a recommended model type from the description (\"query\").\n",
        "\n",
        "If you want, you could implement some more complex logic here, getting the LLM to recommend hyperparameters and things like that. There is also some amount of talking back and forth with itself here, in a chain-of-reasoning style. With a much bigger model, you could start to see some really interesting behavior here. This is what gives DeepSeek its power!\n",
        "\n",
        "#### Note\n",
        "\n",
        "Be careful with how you handle the input dimensions for your models. Convolutional networks want different dimensions than an fully linear network. I recommend making it so that you can feed them the data, and the model itself handles any reshaping needs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the ModelTrainerAgent\n",
        "class ModelTrainerAgent(AssistantAgent):\n",
        "    \"\"\"\n",
        "    An agent that specializes in training machine learning models.\n",
        "    \"\"\"\n",
        "    def __init__(self, name=\"ModelTrainer\", dataset_loader_agent=None, **kwargs):\n",
        "        system_message = (\n",
        "            \"I am a model training assistant. I can train different types of models on loaded datasets.\"\n",
        "        )\n",
        "        super().__init__(name=name, system_message=system_message, **kwargs)\n",
        "        self.dataset_loader_agent = dataset_loader_agent\n",
        "        self.available_models = [\"linear\", \"conv\"] # Define available models\n",
        "        self.model = None # Store the trained model\n",
        "        self.train_loss_list = None\n",
        "        self.val_accuracy_list = None\n",
        "        self.test_accuracy = None\n",
        "\n",
        "    def train_model_on_query(self, dataset_name, query, override=None):\n",
        "        \"\"\"\n",
        "        Select the model type (optionally using an LLM based on a query),\n",
        "        load the dataset, and train the selected model.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Decide on model type: override takes priority\n",
        "            if override in self.available_models:\n",
        "                model_type = override\n",
        "            elif query:\n",
        "                # Part 1: Ask LLM for model recommendation if a query is provided\n",
        "                response = local_model_generate(\n",
        "                    f\"Based on the following dataset description, recommend a model type (linear or convolutional): \\n{query}\"\n",
        "                )\n",
        "                llm_text = response.lower()\n",
        "                if \"linear\" in llm_text:\n",
        "                    model_type = \"linear\"\n",
        "                elif \"conv\" in llm_text or \"convolutional\" in llm_text:\n",
        "                    model_type = \"conv\"\n",
        "                else:\n",
        "                    model_type = \"linear\"  # fallback if unclear\n",
        "            else:\n",
        "                 # Default to linear if no query or override\n",
        "                model_type = \"linear\"\n",
        "\n",
        "\n",
        "            print(f\"LLM recommended model type: {model_type}\" if query else f\"Using specified model type: {model_type}\")\n",
        "\n",
        "\n",
        "            # part 2: Load dataset from DatasetLoaderAgent\n",
        "            dataset = self.dataset_loader_agent.get_dataset(dataset_name)\n",
        "            if not dataset:\n",
        "                return {\"status\": \"error\", \"response\": f\"Dataset '{dataset_name}' is not loaded or does not exist.\"}\n",
        "\n",
        "            # part 3: Extract dataset components\n",
        "            train_features = dataset[\"train_features\"]\n",
        "            train_targets = dataset[\"train_targets\"]\n",
        "            val_features = dataset[\"validation_features\"]\n",
        "            val_targets = dataset[\"validation_targets\"]\n",
        "            test_features = dataset[\"test_features\"]\n",
        "            test_targets = dataset[\"test_targets\"]\n",
        "\n",
        "            # Determine input size (flatten if needed)\n",
        "            if len(train_features.shape) == 3:\n",
        "                indim = train_features.shape[1] * train_features.shape[2]\n",
        "            else:\n",
        "                indim = train_features.shape[1]\n",
        "\n",
        "            print(f\"Training model of type '{model_type}' on dataset '{dataset_name}' with input dimension {indim}.\")\n",
        "            print(f\"Training features shape: {train_features.shape}\")\n",
        "            print(f\"Training targets shape: {train_targets.shape}\")\n",
        "\n",
        "\n",
        "            # part 4: Create model based on type ---\n",
        "            if model_type == \"linear\":\n",
        "                # For solar_flare, the output dim should be 4, not 10\n",
        "                # Assuming 10 is for MNIST which has 10 classes\n",
        "                output_dim = 10 if dataset_name == \"mnist\" else 4\n",
        "                model = LinearClassification(input_dim=indim, output_dim=output_dim)\n",
        "            elif model_type == \"conv\":\n",
        "                 # Reshape features for CNN input if needed\n",
        "                if train_features.ndim == 2 and dataset_name == \"mnist\": # Only reshape if MNIST data (28x28)\n",
        "                    train_features = train_features.reshape(-1, 1, 28, 28)\n",
        "                    val_features = val_features.reshape(-1, 1, 28, 28)\n",
        "                    test_features = test_features.reshape(-1, 1, 28, 28)\n",
        "                elif train_features.ndim != 4 and dataset_name == \"mnist\": # Handle cases where MNIST is already loaded but not in 4D for Conv\n",
        "                     # This might happen if the 'get dataset' was run before defining the Conv model\n",
        "                    train_features = train_features.reshape(-1, 1, 28, 28)\n",
        "                    val_features = val_features.reshape(-1, 1, 28, 28)\n",
        "                    test_features = test_features.reshape(-1, 1, 28, 28)\n",
        "                elif dataset_name != \"mnist\":\n",
        "                    return {\"status\": \"error\", \"response\": f\"Convolutional model is not suitable for dataset '{dataset_name}'.\"}\n",
        "\n",
        "                # For solar_flare, the output dim should be 4, not 10\n",
        "                output_dim = 10 if dataset_name == \"mnist\" else 4\n",
        "                model = ConvClassification(output_dim=output_dim)\n",
        "            else:\n",
        "                return {\"status\": \"error\", \"response\": f\"Unknown model type '{model_type}'.\"}\n",
        "\n",
        "            # part 5: Train the model\n",
        "            self.model, self.train_loss_list, self.val_accuracy_list, self.test_accuracy = train_model(\n",
        "                model, train_features, train_targets, val_features, val_targets,\n",
        "                test_features=test_features, test_targets=test_targets\n",
        "            )\n",
        "\n",
        "            # part 6: Return summary and results ---\n",
        "            response = (\n",
        "                f\"Model '{model_type}' trained successfully on dataset '{dataset_name}'.\\n\"\n",
        "                f\"Final validation accuracy: {self.val_accuracy_list[-1]:.4f}\\n\"\n",
        "                f\"Test accuracy: {self.test_accuracy:.4f}\"\n",
        "            )\n",
        "            print(\"Training response:\", response)\n",
        "\n",
        "            return {\n",
        "                \"role\": \"assistant\",\n",
        "                \"status\": \"success\",\n",
        "                \"model\": self.model,\n",
        "                \"train_loss_list\": self.train_loss_list,\n",
        "                \"val_accuracy_list\": self.val_accuracy_list,\n",
        "                \"test_accuracy\": self.test_accuracy,\n",
        "                \"response\": response\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "                return {\"status\": \"error\", \"response\": f\"An error occurred during training: {str(e)}\"}"
      ],
      "metadata": {
        "id": "tgRgpcVuutg8"
      },
      "id": "tgRgpcVuutg8",
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7b5ef0c7",
      "metadata": {
        "id": "7b5ef0c7"
      },
      "source": [
        "## Train a model on mnist\n",
        "\n",
        "- Get the dataset and train a model on it\n",
        "- You can try specifying, but make sure the LLM can recommend a model type itself\n",
        "- The mnist dataset is very well known, so it should decide on the convolutional model itself (up to the output being bad becasue of the small model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the agents\n",
        "# dataset_loader_agent: Can fetch and preprocess datasets.\n",
        "# model_trainer_agent: Can select, build, and train models.\n",
        "# interface_agent: Is the \"talking\" agent that communicates with the user\n",
        "dataset_loader_agent = DatasetLoaderAgent(name=\"DatasetLoader\")\n",
        "model_trainer_agent = ModelTrainerAgent(name=\"ModelTrainer\", dataset_loader_agent=dataset_loader_agent)\n",
        "interface_agent = InterfaceAgent(name=\"LocalAgent\")\n",
        "\n",
        "# Link the agents\n",
        "interface_agent.set_dataset_loader_agent(dataset_loader_agent)\n",
        "interface_agent.set_model_trainer_agent(model_trainer_agent)\n",
        "\n",
        "# Simulate user message: load dataset\n",
        "messages = [{\"role\": \"user\", \"content\": \"get dataset mnist\"}]\n",
        "response = interface_agent.generate_reply(messages)\n",
        "print(response[\"content\"])\n",
        "print()\n",
        "\n",
        "# Simulate training with LLM model decision\n",
        "messages.append({\"role\": \"user\", \"content\": \"train mnist random\"})\n",
        "response = interface_agent.generate_reply(messages)\n",
        "print(response[\"content\"])\n",
        "print()\n",
        "\n",
        "# Show trained model\n",
        "print(\"Trained model:\")\n",
        "print(model_trainer_agent.model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "lqwsLU_CyLVi",
        "outputId": "cf4d3c6c-2cf3-45ad-a9c3-b90c35bdec7f"
      },
      "id": "lqwsLU_CyLVi",
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset: mnist with 800 training samples\n",
            "Successfully loaded dataset 'mnist'.\n",
            "Training samples: 800, Validation samples: 200, Test samples: 100, Feature dimension: 784.\n",
            "\n",
            "LLM recommended model type: linear\n",
            "Training model of type 'linear' on dataset 'mnist' with input dimension 784.\n",
            "Training features shape: (800, 784)\n",
            "Training targets shape: (800,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [00:03<00:00, 23.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training response: Model 'linear' trained successfully on dataset 'mnist'.\n",
            "Final validation accuracy: 0.8700\n",
            "Test accuracy: 0.9100\n",
            "Successfully trained the model.\n",
            "\n",
            "Trained model:\n",
            "LinearClassification(\n",
            "  (fc1): Linear(in_features=784, out_features=200, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc2): Linear(in_features=200, out_features=10, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9304e7a",
      "metadata": {
        "id": "a9304e7a"
      },
      "source": [
        "## Repeat with solar_flare\n",
        "\n",
        "Now that everything's working, try adding the solar_flare dataset in.\n",
        "\n",
        "- Add solar_flare to the list of available datasets in the datasetloader\n",
        "    - The backend stuff for this is already in the lab 3 code\n",
        "        - This would not be too bad to fit into the agents in principle\n",
        "- The other agents shouldn't need any additional modification, unless you wrote mnist-specific code\n",
        "    - Ignore that the models have mnist in the name, they aren't necessarily mnist specific\n",
        "    - This is easy to do without intending to. The more generalized your code, the better\n",
        "    - Try throwing any errors you get to copilot as a first pass. It's good at generalizing code\n",
        "\n",
        "- In the end, you should be able to tell the agent \"train 'dataset' 'random'\", and it should be able to select and train a linear model if dataset=solar_flare or a convolutional model if dataset=mnist\n",
        "    - Of course, up to LLM inconsistency\n",
        "    - With a good model/more complex logic, it should be able to do this for many different models and any random dataset you throw at it\n",
        "        - Very useful for taking a first crack at a dataset and getting a starting point\n",
        "        - This entire step could also be implemented as a \"copilot, figure out how to add 'dataset'\" command with a model as good as copilot hooked into this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "id": "22f02416",
      "metadata": {
        "id": "22f02416",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "498216a6-b562-46d3-8c0f-c49c58322166"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset: solar_flare with 1000 training samples\n",
            "Successfully loaded dataset 'solar_flare'.\n",
            "Training samples: 1000, Validation samples: 250, Test samples: 139, Feature dimension: 23.\n",
            "LLM recommended model type: linear\n",
            "Training model of type 'linear' on dataset 'solar_flare' with input dimension 23.\n",
            "Training features shape: (1000, 23)\n",
            "Training targets shape: (1000,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [00:02<00:00, 29.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training response: Model 'linear' trained successfully on dataset 'solar_flare'.\n",
            "Final validation accuracy: 0.9920\n",
            "Test accuracy: 0.9928\n",
            "Successfully trained the model.\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE (just more queries to the agents)\n",
        "messages = [{\"role\": \"user\", \"content\": \"get dataset solar_flare\"}]\n",
        "response = interface_agent.generate_reply(messages)\n",
        "print(response[\"content\"])\n",
        "\n",
        "messages.append({\"role\": \"user\", \"content\": \"train solar_flare random\"})\n",
        "response = interface_agent.generate_reply(messages)\n",
        "print(response[\"content\"])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}